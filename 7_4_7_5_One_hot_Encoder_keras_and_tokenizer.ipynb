{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.4_7.5_One_hot_Encoder_keras_and_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbUqv9u8ADLydOnhIHLmXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelounb/NLP-Jason-Brownlee/blob/main/7_4_7_5_One_hot_Encoder_keras_and_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFozoFnz2II6"
      },
      "source": [
        "We can put this together with the one hot() function and encode the words in the document.\n",
        "The complete example is listed below. The vocabulary size is increased by one-third to minimize\n",
        "collisions when hashing words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU6p__K31jLN",
        "outputId": "6d684931-c3f5-40f5-a048-f21ae9cd073c"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "print(words)\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the', 'over', 'jumped', 'quick', 'lazy', 'dog', 'fox', 'brown'}\n",
            "8\n",
            "[9, 5, 9, 6, 5, 3, 9, 2, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT1wbhdP2u6I",
        "outputId": "d2368783-a47b-48e2-9ba4-d2a1a8b8008f"
      },
      "source": [
        "from keras.preprocessing.text import hashing_trick\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHVaE_CD2vfM"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvtiKY2N3smH",
        "outputId": "671b9577-53df-4585-eb7c-46faafc1f0ef"
      },
      "source": [
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "print(len(t.word_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5pPycmNFw7f"
      },
      "source": [
        "Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets. The texts to matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:\n",
        "\n",
        ". binary: Whether or not each word is present in the document. This is the default.\n",
        "\n",
        ". count: The count of each word in the document.\n",
        "\n",
        ". tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "\n",
        ". freq: The frequency of each word as a ratio of words within each document.\n",
        "\n",
        "\n",
        "We can put all of this together with a worked example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY6Fj2jS4ERm",
        "outputId": "f0c2b5da-e8e1-4ebb-8a76-cff50243332c"
      },
      "source": [
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}